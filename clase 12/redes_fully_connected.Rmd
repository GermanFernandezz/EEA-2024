---
title: "R Notebook"
output: html_notebook
---

# Fully connected Neural Network


Esta clase se encuentra basada en el [tutorial de regresión de Tensorflow para R](https://tensorflow.rstudio.com/tutorials/keras/regression)

Las redes neuronales densas (**Fully Connected Neural Networks**) son un tipo de red neuronal artificial donde todas las neuronas de una capa (**layer**) se encuentran conectadas con todas las neuronas de la capa siguiente.

Este tipo de redes neuronales se caracterizan por ser "agnósticas de la estructura o del input" (**structure agnostic**) ya que no es necesario realizar supuestos o tratamientos especiales a los datos de input de la red.

Veamos la arquitectura de una red densa:


## Keras

Vamos a utilizar la librería [__KERAS__](https://keras.rstudio.com/)

### Instalación

Keras utiliza como backend __TensorFlow__. Para poner todo en funcionamiento necesitamos instalar ambas cosas, la secuencia de pasos es:

1. `install.packages("keras")`
2. `library(keras)`
3. `install_keras()`

La función `install_keras()` realiza una instalación por default de basada en el CPU. Es posible realizar una instalación para trabajar con  GPU.
Para una forma de instalación más completa y flexible se pueden seguir las instrucciones del siguiente link: [Instalación Tensorflow R](https://tensorflow.rstudio.com/install/index.html) 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(keras)
library(tensorflow)
# Chequeamos que la instalación de keras y tensorflow sea correcta
tf$constant("Aguante Tensorflow!")
```


## Dataset

```{r}
# levantamos dataset preprocesado
datos_properati <- read.csv("../clase 6/properati_preprocesado_2022.csv")
# creamos nueva variable de superficie descubierta
datos_properati = datos_properati %>%
  mutate(surface_uncovered = surface_total - surface_covered) %>% 
  select(-c(id, surface_total, precio_en_miles))
```

```{r}
datos_properati %>% head()
```

## Modelo lineal

```{r}
# fijamos semilla
set.seed(22)
# Partición Train y Test, indicando proporción
split_modelo_lineal <- initial_split(datos_properati, prop = 0.75)
train_modelo_lineal <- training(split_modelo_lineal)
test_modelo_lineal <- testing(split_modelo_lineal)
```

```{r}
ggplot(train_modelo_lineal, aes(x=surface_covered, y=price)) +
  geom_point(alpha=0.25) +
  theme_bw() +
  labs(title="Relación Superficie Cubierta y Precio", x="Superficie Cubierta", y="Precio")

ggplot(train_modelo_lineal, aes(x=surface_covered, y=price, color=property_type)) +
  geom_point(alpha=0.5) +
  theme_bw() +
  facet_wrap(~property_type) +
  labs(title="Relación Superficie Cubierta y Precio", x="Superficie Cubierta", y="Precio")
```


```{r}
# Entrenamos un modelo con 2 variables
modelo_lineal_basico <- lm(formula = price ~ surface_covered + property_type, data = train_modelo_lineal)

# Entrenamos un modelo multiple
modelo_lineal_todas <- lm(formula = price ~., data = train_modelo_lineal)
```

```{r}
modelos_lineales = list(modelo_lineal_basico = modelo_lineal_basico, modelo_lineal_todas = modelo_lineal_todas)

lista_predicciones_training = map(.x = modelos_lineales, .f = augment) 

map_dfr(.x = lista_predicciones_training, .f = metrics, truth = price, estimate = .fitted, .id="modelo") %>%
        arrange(.estimate) %>% 
        mutate(.estimate= round(.estimate, digits = 2))
```

```{r}
lista_predicciones_testing = map(.x = modelos_lineales, .f = augment, newdata = test_modelo_lineal) 

map_dfr(.x = lista_predicciones_testing, .f = metrics, truth = price, estimate = .fitted, .id="modelo") %>%
        arrange(.estimate) %>% 
        mutate(.estimate= round(.estimate, digits = 2))
```


# Redes

```{r}
library(recipes)
properati_redes <- recipe(price ~ ., datos_properati) %>%
  step_dummy(property_type, one_hot = TRUE) %>%
  step_dummy(l3, one_hot = TRUE) %>%
  prep() %>%
  bake(new_data = NULL)
```


```{r}
# Partición Train y Test, indicando proporción
split_redes <- initial_split(properati_redes, prop = 0.75)
train_redes <- training(split_redes)
test_redes <- testing(split_redes)
```


```{r}
variables_train <- train_redes %>% select(-price)
variables_test <- test_redes %>% select(-price)

precio_train <- train_redes %>% select(price)
precio_test <- test_redes %>% select(price)
```

## Normalización

La normalización ayuda a que el proceso de optimización sea más rápido. Aunque rara vez es estrictamente necesario es una práctica muy usual en el trabajo con redes neuronales.

```{r, warning=FALSE}
normalizer <- layer_normalization(axis = -1L)

normalizer %>% adapt(as.matrix(variables_train))

normalizer$mean
```
## Red sencilla

```{r}
variables_red_sencila <-  variables_train %>% select(surface_covered, property_type_Casa,
                                                     property_type_Departamento, property_type_PH) %>% as.matrix()
normalizador_sencillo <- layer_normalization(input_shape = shape(4), axis = NULL)
normalizador_sencillo %>% adapt(variables_red_sencila)
```


```{r}
red_sencilla <- keras_model_sequential() %>%
  normalizador_sencillo() %>%
  layer_dense(units = 1)

summary(red_sencilla)
```

## Optimización y Función de Pérdida

Para compilar la red vamos a utilizar la función `compile`. Como mínimo es necesario definir:

* `optimizer`: el método de optimización

* `loss`: la función de costo/pérdida


El **método de optimización** es la manera en la cual se realiza el update (actualización) de los parámetros de la red.
Un listado de los métodos de optimización se encuentra en: [Métodos de Optimización de Tensorflow](https://tensorflow.rstudio.com/reference/keras/index.html#optimizers). Se encuentran implementados desde los más simples como Descenso por el Gradiente Estocástico o Stochastic Gradient Descent hasta los más novedosos y comúnmente utilizados como ADAM (Adaptative Moment Estimation).

Cada uno de estos métodos tienen argumentos para configurar pero nos vamos a preocupar sólo por el *learning_rate*

La **función de costo/pérdida** es la función de costo

Una lista de las funciones de pérdida se encuentra en: [Funciones de pérdida de Tensorflow](https://tensorflow.rstudio.com/reference/keras/loss-functions.html#loss-functions-1). Para problemas de regresión podemos utilizar funciones como: MSE, MAE o MAPE.

En nuestro caso vamos a estar utilizando el optimizador ADAM (por convención) y la función de pérdida de MSE/ECM para que sea similar a los modelos lineales


```{r}
red_sencilla %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```


```{r}
historia_red_sencilla <- red_sencilla %>% fit(
  variables_red_sencila,
  as.matrix(precio_train),
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```

```{r}
plot(historia_red_sencilla)
```


```{r}
red_superficie_cubierta %>%
  evaluate(
    as.matrix(variables_test$surface_covered),
    as.matrix(precio_test),
    #verbose = 0
  )
```
## Red sencilla profunda

```{r}
red_sencilla_profunda <- keras_model_sequential() %>%
    normalizador_sencillo() %>%
    layer_dense(64, activation = 'linear') %>% 
    layer_dense(32, activation = 'linear') %>%
    layer_dense(1)

red_sencilla_profunda
```

```{r}
red_sencilla_profunda %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```

```{r}
historia_red_sencilla_profunda <- red_sencilla_profunda %>% fit(
  variables_red_sencila,
  as.matrix(precio_train),
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```

```{r}
plot(historia_red_sencilla_profunda)
```


## Red sencilla profunda con activación no lineal

### Funciones de activación

Para este modelo utilizamos dos funciones de activación: 

- Rectified Linear Unit: $$f(x)=max(0,x)$$

Definidas en código y gráficamente:


```{r}
relu <- function(x) ifelse(x >= 0, x, 0)

relu_df = data.frame(x= seq(from=-2, to=2, by=0.25)) %>% 
  mutate(relu = relu(x)) 

relu_plot = ggplot(relu_df, aes(x=x, y=relu))+
  geom_line(size=1,  colour='steelblue') +
  ggtitle("ReLU")+
  theme_bw()

relu_plot
```

https://tensorflow.rstudio.com/reference/keras/activation_relu

```{r}
red_sencilla_profunda_no_lineal <- keras_model_sequential() %>%
    normalizador_sencillo() %>%
    layer_dense(64, activation = 'relu') %>% 
    layer_dense(32, activation = 'relu') %>%
    layer_dense(1)

red_sencilla_profunda_no_lineal
```

```{r}
red_sencilla_profunda_no_lineal %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```

```{r}
historia_red_sencilla_profunda_no_lineal <- red_sencilla_profunda_no_lineal %>% fit(
  variables_red_sencila,
  as.matrix(precio_train),
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```

```{r}
plot(historia_red_sencilla_profunda_no_lineal)
```


__ReLu__ es la función de activación que más se utiliza en la actualidad en las hidden layers. 


```{r}
build_and_compile_model <- function(norm) {
  model <- keras_model_sequential() %>%
    norm() %>%
    layer_dense(64, activation = 'relu') %>% 
    layer_dense(32, activation = 'relu') %>%
    layer_dense(1)

  model %>% compile(
    loss = 'mean_squared_error',
    optimizer = optimizer_adam(0.05)
  )

  model
}
```

```{r}
red_profunda <- build_and_compile_model(normalizer)
red_profunda
```


```{r}
historia <- red_profunda %>% fit(
  as.matrix(variables_train),
  as.matrix(precio_train),
  validation_split = 0.2,
  verbose = 0,
  epochs = 100,
  seed = 1992
)
```

```{r}
plot(historia) + theme_bw()
```




```{r}
y_pred <- predict(red_profunda, as.matrix(variables_test))
```

```{r}
rmse_vec(truth = precio_test$price , estimate = c(y_pred))
```

```{r}
ggplot(data.frame(pred = as.numeric(y_pred), precio = precio_test$price)) +
  geom_point(aes(x = pred, y = precio), alpha=0.9) +
  geom_abline(intercept = 0, slope = 1, color = "blue")
```

