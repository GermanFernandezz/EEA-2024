---
title: "Redes Neuronales"
author: "Juan Barriola, Azul Villanueva y Franco Mastelli"
date: "26 de noviembre de 2022"
---

# Redes Neuronales

La idea central de esta clase es presentar a las redes neuronales como modelos muy flexibles y potentes que se pueden considerar como **aproximadores universales**. Se desarrollarán de manera superficial conceptos vinculados a la construcción de una red neuronal densa, funciones de activación y algunas conceptos clave del entrenamiento/fitting de una red.

Esta clase se encuentra basada en el [tutorial de regresión de Tensorflow para R](https://tensorflow.rstudio.com/tutorials/keras/regression)

## Keras

Para trabajar con redes neuronales vamos a utilizar la librería [__KERAS__](https://keras.rstudio.com/)

### Instalación

Keras utiliza como backend __TensorFlow__. Para poner todo en funcionamiento necesitamos instalar ambas cosas, la secuencia de pasos es:

1. `install.packages("keras")`
2. `library(keras)`
3. `install_keras()`

La función `install_keras()` realiza una instalación por default de basada en el CPU. Es posible realizar una instalación para trabajar con  GPU.
Para una forma de instalación más completa y flexible se pueden seguir las instrucciones del siguiente link: [Instalación Tensorflow R](https://tensorflow.rstudio.com/install/index.html) 

```{r, message=FALSE, warning=FALSE}
# Importamos las librerías
library(tidyverse)
library(tidymodels)
library(keras)
library(tensorflow)
# Chequeamos que la instalación de keras y tensorflow sea correcta
tf$constant("Aguante Tensorflow!")
# Fijamos semilla
set.seed(22)
```

## Dataset

Vamos a trabajar sobre el dataset de Properati, que ya utilizamos en clases anteriores 

```{r}
# Levantamos dataset preprocesado
datos_properati <- read.csv("../clase 6/properati_preprocesado_2022.csv")
# Creamos nueva variable de superficie descubierta
datos_properati = datos_properati %>%
  mutate(surface_uncovered = surface_total - surface_covered) %>% 
  # Eliminamos algunas variables
  select(-c(id, surface_total, precio_en_miles))
```

Observamos los primeros registros de la tabla:

```{r}
datos_properati %>% head()
```

## Modelo lineal

Vamos a realizar dos modelos lineales para establecer un baseline de performance para comparar los modelos de redes neuronales.
Comenzamos observando la relación entre precio y superficie cubierta con apertura por tipo de propiedad:

```{r}
ggplot(datos_properati, aes(x=surface_covered, y=price, color=property_type)) +
  geom_point(alpha=0.5) +
  theme_bw() +
  facet_wrap(~property_type) +
  labs(title="Relación Superficie Cubierta y Precio", x="Superficie Cubierta", y="Precio")
```

Ahora realizamos un split entre datos de entrenamiento (75%) y evaluación (25%)

```{r}
# Partición Train y Test, indicando proporción
split_modelo_lineal <- initial_split(datos_properati, prop = 0.75)
train_modelo_lineal <- training(split_modelo_lineal)
test_modelo_lineal <- testing(split_modelo_lineal)
```

Vamos a realizar dos modelos lineales clásicos:

**Modelo Básico**

$E(precio) = \beta_0+\beta_1SuperficieCubierta + \beta_2TipoPropiedad$

**Modelo Completo**

$E(precio) = \beta_0+ \beta_1SuperficieCubierta + \beta_2TipoPropiedad + \beta_3SuperficieDescubierta + \beta_4L3 + \beta_5Habitaciones + \beta_6Baños$

Entrenamos ambos modelos

```{r}
# Entrenamos el modelo básico
modelo_lineal_basico <- lm(formula = price ~ surface_covered + property_type, data = train_modelo_lineal)

# Entrenamos el modelo completo
modelo_lineal_completo <- lm(formula = price ~., data = train_modelo_lineal)
```

Predecimos los valores sobre el dataset de train y obtenemos los valores de RMSE y MAE

```{r}
# Listado de modelos
modelos_lineales = list(modelo_lineal_basico = modelo_lineal_basico, modelo_lineal_completo = modelo_lineal_completo)

# Realizamos predicciones en train
lista_predicciones_training = map(.x = modelos_lineales, .f = augment) 

# Obtenemos las métricas de performance en train
map_dfr(.x = lista_predicciones_training, .f = metrics, truth = price, estimate = .fitted, .id="modelo") %>%
        filter(.metric!="rsq") %>% 
        arrange(.metric, modelo) %>% 
        mutate(.estimate= round(.estimate, digits = 2))
```

Predecimos los valores sobre el dataset de test y obtenemos los valores de RMSE y MAE

```{r}
# Realizamos predicciones en test
lista_predicciones_testing = map(.x = modelos_lineales, .f = augment, newdata = test_modelo_lineal) 

# Obtenemos las métricas de performance en train
map_dfr(.x = lista_predicciones_testing, .f = metrics, truth = price, estimate = .fitted, .id="modelo") %>%
        filter(.metric!="rsq") %>% 
        arrange(.metric, modelo) %>% 
        mutate(.estimate= round(.estimate, digits = 2))
```

## Redes Neuronales Densas

Las redes neuronales densas (**Fully Connected Neural Networks**) son un tipo de red neuronal artificial donde todas las neuronas de una capa (**layer**) se encuentran conectadas con todas las neuronas de la capa siguiente.

Este tipo de redes neuronales no realizan supuestos sobre la estructura de los datos de input de la red: cada observación o ejemplo es representada por un vector numérico. 

### Preprocesamiento de datos

En nuestro caso va a ser necesario realizar una transformación de las variables categóricas *property_type* y *l3*. Para el preprocesamiento vamos a utilizar la librería `recipes` para seguir lo planteado por el tutorial previamente citado. 

En esta preparación vamos a transformar a una representación numérica a las variables *property_type* y *l3* mediante one-hot encoding

```{r}
library(recipes)
properati_redes <- recipe(price ~ ., datos_properati) %>%
  # Realizamos one-hot encoding de la variable tipo de propiedad
  step_dummy(property_type, one_hot = TRUE) %>%
  # Realizamos one-hot encoding de la variable l3
  step_dummy(l3, one_hot = TRUE) %>%
  prep() %>%
  bake(new_data = NULL)

properati_redes %>% head()
```

Ahora realizamos un split entre datos de entrenamiento (75%) y evaluación (25%) para los datos transformados para redes neuronales

```{r}
# Partición Train y Test, indicando proporción
split_redes <- initial_split(properati_redes, prop = 0.75)
train_redes <- training(split_redes)
test_redes <- testing(split_redes)
```

Adicionalmente separamos nuestras variables predictoras de la variable a predecir (precio)

```{r}
# Variables predictoras
variables_train <- train_redes %>% select(-price)
variables_test <- test_redes %>% select(-price)

# Variable a predecir
precio_train <- train_redes %>% select(price) %>% as.matrix()
precio_test <- test_redes %>% select(price) %>% as.matrix()
```

### Normalización

La normalización ayuda a que el proceso de optimización sea más rápido. Aunque rara vez es estrictamente necesario es una práctica muy usual en el trabajo con redes neuronales.

```{r, warning=FALSE}
capa_normalización <- layer_normalization(axis = -1L)

capa_normalización %>% adapt(as.matrix(variables_train))

capa_normalización$mean
```
## Red sencilla

Vamos a comenzar realizando una red muy sencilla:

  * Vamos a usar las variables de *surface_covered* y *property_type* 
  * Vamos a usar sólo una capa con una única neurona

Comenzamos preparando los datos para esta red sencilla y generando un normalizador específico

```{r}
# Obtenemos las variables de interés
variables_train_red_sencila <-  variables_train %>% select(surface_covered, property_type_Casa,
                                                     property_type_Departamento, property_type_PH) %>% as.matrix()

variables_test_red_sencila <-  variables_test %>% select(surface_covered, property_type_Casa,
                                                     property_type_Departamento, property_type_PH) %>% as.matrix()
# Generamos el normalizador de la red sencilla
normalizador_sencillo <- layer_normalization(input_shape = shape(4), axis = NULL)
normalizador_sencillo %>% adapt(variables_train_red_sencila)
```

### Construcción de la red

Para armar el modelo primero definimos el tipo de modelo. Para eso usamos `keras_model_sequential()` que nos permite simplemente apilar capas de la red. 

- En la primera capa tenemos que aclarar el input_shape. En este caso es **unidimensional** (lo aplanamos previamente), pero podría ser un tensor de cualquier dimensión (!!)

- Las capas se agregan con pipes `%>%`

- La última capa tiene la misma cantidad de unidades que categorías nuestro output. La salida del modelo es un vector que asigna una probabilidad a cada una da las categorías

- En cada capa tenemos que definir una función de activación


```{r}
red_sencilla <- keras_model_sequential() %>%
  normalizador_sencillo() %>%
  layer_dense(units = 1)

summary(red_sencilla)
```

Cada **neurona** de una capa tiene dos parámetros: los pesos (weights) y el sesgo (bias). Veamos la fórmula matemática de nuestra red para una sola observación 

$Z_{1x1} = X_{1x4}\cdot W_{4x1} + b_{1x1}$

Esto lo podemos reexpresar de la siguiente manera:

$$
z_i = \begin{pmatrix}x_{1i} & x_{2i} & x_{3i} & x_{4i}\end{pmatrix} \cdot \begin{pmatrix} w_1 \\ w_2 \\ w_3 \\ w_4 \end{pmatrix} + b
\\
z = b + w_1 \cdot x_{1i} + w_2 \cdot x_{2i} + w_3 \cdot x_{3i} + w_4 \cdot x_{4i}

$$

Vemos en este caso que la red tendrá que estimar 5 parámetros: 4 pesos y 1 sesgo

### Optimización y Función de Pérdida

Para compilar la red vamos a utilizar la función `compile`. Como mínimo es necesario definir:

* `optimizer`: el método de optimización

* `loss`: la función de costo/pérdida

El **método de optimización** es la manera en la cual se realiza el update (actualización) de los parámetros de la red.
Un listado de los métodos de optimización se encuentra en: [Métodos de Optimización de Tensorflow](https://tensorflow.rstudio.com/reference/keras/index.html#optimizers). Se encuentran implementados desde los más simples como Descenso por el Gradiente Estocástico o Stochastic Gradient Descent hasta los más novedosos y comúnmente utilizados como ADAM (Adaptative Moment Estimation).

Cada uno de estos métodos tienen argumentos para configurar pero nos vamos a preocupar sólo por el *learning_rate*

La **función de costo/pérdida** es la función de costo

Una lista de las funciones de pérdida se encuentra en: [Funciones de pérdida de Tensorflow](https://tensorflow.rstudio.com/reference/keras/loss-functions.html#loss-functions-1). Para problemas de regresión podemos utilizar funciones como: MSE, MAE o MAPE.

En nuestro caso vamos a estar utilizando el optimizador ADAM (por convención) y la función de pérdida de MSE/ECM para que sea similar a los modelos lineales

```{r}
#Compilamos la red
red_sencilla %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```

### Entrenamiento

Para ajustar el modelo usamos la función `fit()`, acá necesitamos pasar los siguientes parámetros:

- El array con los datos de entrenamiento
- El array con los outputs
- `epochs`: Cuantas veces va a recorrer el dataset de entrenamiento
- `batch_size`: de a cuantas imagenes va a mirar en cada iteración del backpropagation
- `validation_split`: Hacemos un split en train y validation para evaluar las métricas.

```{r}
historia_red_sencilla <- red_sencilla %>% fit(
  variables_train_red_sencila,
  as.matrix(precio_train),
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```

```{r}
# Generamos una función para graficar la historia de la red
graficar_historia <- function(historia_red, titulo){
  plot(historia_red) +
    theme_bw() + 
    labs(title = titulo, x = "Época", y="MSE") +
    scale_y_continuous(labels = scales::comma)
}
```



```{r}
graficar_historia(historia_red = historia_red_sencilla, titulo = "Historia Red Sencilla")
```
```{r}
augment_redes <- function(modelo_red, matriz_variables, variable_target) {
  y_pred <- predict(modelo_red, matriz_variables) %>% as.vector()
  df_predicciones <- tibble(y = as.vector(variable_target), y_pred = y_pred)
  return(df_predicciones)
}
```

```{r}
red_sencilla %>%
  evaluate(
    variables_test_red_sencila,
    precio_test,
    verbose = 0 
  ) %>% sqrt()
```


## Red sencilla profunda

Comentario breve sobre redes profundas

Explicación sobre layer dense

```{r}
red_sencilla_profunda <- keras_model_sequential() %>%
    normalizador_sencillo() %>%
    layer_dense(64, activation = 'linear') %>% 
    layer_dense(32, activation = 'linear') %>%
    layer_dense(1)

red_sencilla_profunda
```

Insertar algún diagrama
Hacer el cálculo de los parámetros

```{r}
red_sencilla_profunda %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```

```{r}
historia_red_sencilla_profunda <- red_sencilla_profunda %>% fit(
  variables_train_red_sencila,
  precio_train,
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```

```{r}
graficar_historia(historia_red = historia_red_sencilla_profunda, titulo = "Historia Red Sencilla Profunda")
```

```{r}
red_sencilla_profunda %>%
  evaluate(
    variables_test_red_sencila,
    precio_test,
    verbose = 0 
  ) %>% sqrt()
```


## Red sencilla profunda con activación no lineal

### Funciones de activación

Las funciones de activación 

Para este modelo vamos a utilizar utilizamos la función de activación ReLu: 

- Rectified Linear Unit: $$f(x)=max(0,x)$$

Definida en código y gráficamente:


```{r}
relu <- function(x) ifelse(x >= 0, x, 0)

relu_df = data.frame(x= seq(from=-2, to=2, by=0.25)) %>% 
  mutate(relu = relu(x)) 

relu_plot = ggplot(relu_df, aes(x=x, y=relu))+
  geom_line(size=1,  colour='steelblue') +
  ggtitle("ReLU")+
  theme_bw()

relu_plot
```

https://tensorflow.rstudio.com/reference/keras/activation_relu

```{r}
red_sencilla_profunda_no_lineal <- keras_model_sequential() %>%
    normalizador_sencillo() %>%
    layer_dense(64, activation = 'relu') %>% 
    layer_dense(32, activation = 'relu') %>%
    layer_dense(1)

red_sencilla_profunda_no_lineal
```

```{r}
red_sencilla_profunda_no_lineal %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```

```{r}
historia_red_sencilla_profunda_no_lineal <- red_sencilla_profunda_no_lineal %>% fit(
  variables_train_red_sencila,
  precio_train,
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```


```{r}
graficar_historia(historia_red = historia_red_sencilla_profunda_no_lineal, titulo = "Historia Red Sencilla Profunda No Lineal")
```

```{r}
red_sencilla_profunda_no_lineal %>%
  evaluate(
    variables_test_red_sencila,
    precio_test,
    verbose = 0 
  ) %>% sqrt()
```


## Red profunda con todas las variables
fgsd

```{r}
red_profunda <- keras_model_sequential() %>%
    capa_normalización() %>%
    layer_dense(64, activation = 'relu') %>% 
    layer_dense(32, activation = 'relu') %>%
    layer_dense(1)

red_profunda
```

```{r}
red_profunda %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```

```{r}
historia_red_profunda <- red_profunda %>% fit(
  as.matrix(variables_train),
  precio_train,
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```


```{r}
graficar_historia(historia_red = historia_red_profunda, titulo = "Historia Red Profunda Todas Variables")
```



```{r}
red_profunda %>%
  evaluate(
    as.matrix(variables_test),
    precio_test,
    verbose = 0 
  ) %>% sqrt()
```
```{r}
pred_red_profunda_train <- augment_redes(red_profunda, as.matrix(variables_train), precio_train)

pred_red_profunda_test <- augment_redes(red_profunda, as.matrix(variables_test), precio_test)
```
```{r}
metrics(pred_red_profunda_train, truth = y, estimate = y_pred) %>% 
  mutate(.estimate= round(.estimate, digits = 2))
```

```{r}
metrics(pred_red_profunda_test, truth = y, estimate = y_pred) %>% 
  mutate(.estimate= round(.estimate, digits = 2))
```

```{r}
ggplot(pred_red_profunda_test, aes(x=y, y=y_pred)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue")
```

```{r}
ggplot(lista_predicciones_testing$modelo_lineal_todas, aes(x=price, y=.fitted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue")
```

