---
title: "R Notebook"
output: html_notebook
---

# Fully connected Neural Network


Esta clase se encuentra basada en el [tutorial de regresión de Tensorflow para R](https://tensorflow.rstudio.com/tutorials/keras/regression)

Las redes neuronales densas (**Fully Connected Neural Networks**) son un tipo de red neuronal artificial donde todas las neuronas de una capa (**layer**) se encuentran conectadas con todas las neuronas de la capa siguiente.

Este tipo de redes neuronales se caracterizan por ser "agnósticas de la estructura o del input" (**structure agnostic**) ya que no es necesario realizar supuestos o tratamientos especiales a los datos de input de la red.

Veamos la arquitectura de una red densa:


## Keras

Vamos a utilizar la librería [__KERAS__](https://keras.rstudio.com/)

### Instalación

Keras utiliza como backend __TensorFlow__. Para poner todo en funcionamiento necesitamos instalar ambas cosas, la secuencia de pasos es:

1. `install.packages("keras")`
2. `library(keras)`
3. `install_keras()`

La función `install_keras()` realiza una instalación por default de basada en el CPU. Es posible realizar una instalación para trabajar con  GPU.
Para una forma de instalación más completa y flexible se pueden seguir las instrucciones del siguiente link: [Instalación Tensorflow R](https://tensorflow.rstudio.com/install/index.html) 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(keras)
library(tensorflow)
# Chequeamos que la instalación de keras y tensorflow sea correcta
tf$constant("Aguante Tensorflow!")
```


## Dataset

```{r}
# levantamos dataset preprocesado
datos_properati <- read.csv("../clase 6/properati_preprocesado_2022.csv")
# creamos nueva variable de superficie descubierta
datos_properati = datos_properati %>%
  mutate(surface_uncovered = surface_total - surface_covered) %>% 
  select(-c(id, surface_total, precio_en_miles))
```

```{r}
datos_properati %>% head()
```

## Modelo lineal

```{r}
# fijamos semilla
set.seed(22)
# Partición Train y Test, indicando proporción
split_modelo_lineal <- initial_split(datos_properati, prop = 0.75)
train_modelo_lineal <- training(split_modelo_lineal)
test_modelo_lineal <- testing(split_modelo_lineal)
```

```{r}
ggplot(train_modelo_lineal, aes(x=surface_covered, y=price, color=property_type)) +
  geom_point(alpha=0.5) +
  theme_bw() +
  facet_wrap(~property_type) +
  labs(title="Relación Superficie Cubierta y Precio", x="Superficie Cubierta", y="Precio")
```


```{r}
# Entrenamos un modelo con 2 variables
modelo_lineal_basico <- lm(formula = price ~ surface_covered + property_type, data = train_modelo_lineal)

# Entrenamos un modelo multiple
modelo_lineal_todas <- lm(formula = price ~., data = train_modelo_lineal)
```

```{r}
modelos_lineales = list(modelo_lineal_basico = modelo_lineal_basico, modelo_lineal_todas = modelo_lineal_todas)

lista_predicciones_training = map(.x = modelos_lineales, .f = augment) 

map_dfr(.x = lista_predicciones_training, .f = metrics, truth = price, estimate = .fitted, .id="modelo") %>%
        arrange(.metric, modelo) %>% 
        mutate(.estimate= round(.estimate, digits = 2))
```

```{r}
lista_predicciones_testing = map(.x = modelos_lineales, .f = augment, newdata = test_modelo_lineal) 

map_dfr(.x = lista_predicciones_testing, .f = metrics, truth = price, estimate = .fitted, .id="modelo") %>%
        arrange(.metric, modelo) %>% 
        mutate(.estimate= round(.estimate, digits = 2))
```


# Redes

```{r}
library(recipes)
properati_redes <- recipe(price ~ ., datos_properati) %>%
  step_dummy(property_type, one_hot = TRUE) %>%
  step_dummy(l3, one_hot = TRUE) %>%
  prep() %>%
  bake(new_data = NULL)
```


```{r}
# Partición Train y Test, indicando proporción
split_redes <- initial_split(properati_redes, prop = 0.75)
train_redes <- training(split_redes)
test_redes <- testing(split_redes)
```


```{r}
variables_train <- train_redes %>% select(-price)
variables_test <- test_redes %>% select(-price)

precio_train <- train_redes %>% select(price) %>% as.matrix()
precio_test <- test_redes %>% select(price) %>% as.matrix()
```

## Normalización

La normalización ayuda a que el proceso de optimización sea más rápido. Aunque rara vez es estrictamente necesario es una práctica muy usual en el trabajo con redes neuronales.

```{r, warning=FALSE}
normalizer <- layer_normalization(axis = -1L)

normalizer %>% adapt(as.matrix(variables_train))

normalizer$mean
```
## Red sencilla

```{r}
variables_train_red_sencila <-  variables_train %>% select(surface_covered, property_type_Casa,
                                                     property_type_Departamento, property_type_PH) %>% as.matrix()

variables_test_red_sencila <-  variables_test %>% select(surface_covered, property_type_Casa,
                                                     property_type_Departamento, property_type_PH) %>% as.matrix()
normalizador_sencillo <- layer_normalization(input_shape = shape(4), axis = NULL)
normalizador_sencillo %>% adapt(variables_train_red_sencila)
```


```{r}
red_sencilla <- keras_model_sequential() %>%
  normalizador_sencillo() %>%
  layer_dense(units = 1)

summary(red_sencilla)
```

## Optimización y Función de Pérdida

Para compilar la red vamos a utilizar la función `compile`. Como mínimo es necesario definir:

* `optimizer`: el método de optimización

* `loss`: la función de costo/pérdida


El **método de optimización** es la manera en la cual se realiza el update (actualización) de los parámetros de la red.
Un listado de los métodos de optimización se encuentra en: [Métodos de Optimización de Tensorflow](https://tensorflow.rstudio.com/reference/keras/index.html#optimizers). Se encuentran implementados desde los más simples como Descenso por el Gradiente Estocástico o Stochastic Gradient Descent hasta los más novedosos y comúnmente utilizados como ADAM (Adaptative Moment Estimation).

Cada uno de estos métodos tienen argumentos para configurar pero nos vamos a preocupar sólo por el *learning_rate*

La **función de costo/pérdida** es la función de costo

Una lista de las funciones de pérdida se encuentra en: [Funciones de pérdida de Tensorflow](https://tensorflow.rstudio.com/reference/keras/loss-functions.html#loss-functions-1). Para problemas de regresión podemos utilizar funciones como: MSE, MAE o MAPE.

En nuestro caso vamos a estar utilizando el optimizador ADAM (por convención) y la función de pérdida de MSE/ECM para que sea similar a los modelos lineales


```{r}
red_sencilla %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```


```{r}
historia_red_sencilla <- red_sencilla %>% fit(
  variables_train_red_sencila,
  as.matrix(precio_train),
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```

```{r}
graficar_historia <- function(historia_red, titulo){
  plot(historia_red) +
    theme_bw() + 
    labs(title = titulo, x = "Época", y="MSE") +
    scale_y_continuous(labels = scales::comma)
}
```



```{r}
graficar_historia(historia_red = historia_red_sencilla, titulo = "Historia Red Sencilla")
```
```{r}
augment_redes <- function(modelo_red, matriz_variables, variable_target) {
  y_pred <- predict(modelo_red, matriz_variables) %>% as.vector()
  df_predicciones <- tibble(y = as.vector(variable_target), y_pred = y_pred)
  return(df_predicciones)
}
```

```{r}
red_sencilla %>%
  evaluate(
    variables_test_red_sencila,
    precio_test,
    verbose = 0 
  ) %>% sqrt()
```
```{r}
pred
```


## Red sencilla profunda

Comentario breve sobre redes profundas

Explicación sobre layer dense

```{r}
red_sencilla_profunda <- keras_model_sequential() %>%
    normalizador_sencillo() %>%
    layer_dense(64, activation = 'linear') %>% 
    layer_dense(32, activation = 'linear') %>%
    layer_dense(1)

red_sencilla_profunda
```

Insertar algún diagrama
Hacer el cálculo de los parámetros

```{r}
red_sencilla_profunda %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```

```{r}
historia_red_sencilla_profunda <- red_sencilla_profunda %>% fit(
  variables_train_red_sencila,
  precio_train,
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```

```{r}
graficar_historia(historia_red = historia_red_sencilla_profunda, titulo = "Historia Red Sencilla Profunda")
```

```{r}
red_sencilla_profunda %>%
  evaluate(
    variables_test_red_sencila,
    precio_test,
    verbose = 0 
  ) %>% sqrt()
```


## Red sencilla profunda con activación no lineal

### Funciones de activación

Las funciones de activación 

Para este modelo vamos a utilizar utilizamos la función de activación ReLu: 

- Rectified Linear Unit: $$f(x)=max(0,x)$$

Definida en código y gráficamente:


```{r}
relu <- function(x) ifelse(x >= 0, x, 0)

relu_df = data.frame(x= seq(from=-2, to=2, by=0.25)) %>% 
  mutate(relu = relu(x)) 

relu_plot = ggplot(relu_df, aes(x=x, y=relu))+
  geom_line(size=1,  colour='steelblue') +
  ggtitle("ReLU")+
  theme_bw()

relu_plot
```

https://tensorflow.rstudio.com/reference/keras/activation_relu

```{r}
red_sencilla_profunda_no_lineal <- keras_model_sequential() %>%
    normalizador_sencillo() %>%
    layer_dense(64, activation = 'relu') %>% 
    layer_dense(32, activation = 'relu') %>%
    layer_dense(1)

red_sencilla_profunda_no_lineal
```

```{r}
red_sencilla_profunda_no_lineal %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```

```{r}
historia_red_sencilla_profunda_no_lineal <- red_sencilla_profunda_no_lineal %>% fit(
  variables_train_red_sencila,
  precio_train,
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```


```{r}
graficar_historia(historia_red = historia_red_sencilla_profunda_no_lineal, titulo = "Historia Red Sencilla Profunda No Lineal")
```

```{r}
red_sencilla_profunda_no_lineal %>%
  evaluate(
    variables_test_red_sencila,
    precio_test,
    verbose = 0 
  ) %>% sqrt()
```

## Red profunda con todas las variables


```{r}
red_profunda <- keras_model_sequential() %>%
    normalizer() %>%
    layer_dense(64, activation = 'relu') %>% 
    layer_dense(32, activation = 'relu') %>%
    layer_dense(1)

red_profunda
```

```{r}
red_profunda %>% compile(
  optimizer = optimizer_adam(learning_rate = 0.1),
  loss = 'mean_squared_error'
)
```

```{r}
historia_red_profunda <- red_profunda %>% fit(
  as.matrix(variables_train),
  precio_train,
  epochs = 100,
  # Suppress logging.
  verbose = 0,
  validation_split = 0.2,
  seed=1992
)
```


```{r}
graficar_historia(historia_red = historia_red_profunda, titulo = "Historia Red Profunda Todas Variables")
```

```{r}
red_profunda %>%
  evaluate(
    as.matrix(variables_test),
    precio_test,
    verbose = 0 
  ) %>% sqrt()
```
```{r}
pred_red_profunda_train <- augment_redes(red_profunda, as.matrix(variables_train), precio_train)

pred_red_profunda_test <- augment_redes(red_profunda, as.matrix(variables_test), precio_test)
```
```{r}
metrics(pred_red_profunda_train, truth = y, estimate = y_pred) %>% 
  mutate(.estimate= round(.estimate, digits = 2))
```

```{r}
metrics(pred_red_profunda_test, truth = y, estimate = y_pred) %>% 
  mutate(.estimate= round(.estimate, digits = 2))
```

```{r}
ggplot(pred_red_profunda_test, aes(x=y, y=y_pred)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue")
```

```{r}
ggplot(lista_predicciones_testing$modelo_lineal_todas, aes(x=price, y=.fitted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "blue")
```

